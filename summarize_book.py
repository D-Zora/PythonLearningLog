import nltk
import os
from nltk.tokenize import sent_tokenize
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer

# ä¸‹è½½ punkt åˆ†è¯æ¨¡å‹ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
nltk.download('punkt')

# è¯»å–txtæ–‡ä»¶
def read_book(txt_path):
    with open(txt_path, 'r', encoding='utf-8') as f:
        return f.read()

# æ‹†åˆ†ä¸ºå¥å­ä¸æ®µè½
def split_sentences(text):
    return sent_tokenize(text)

def split_paragraphs(text):
    return [p.strip() for p in text.split('\n') if len(p.strip()) > 50]

# æå–å…³é”®å¥ï¼ˆä½œä¸ºæ ‡é¢˜ï¼‰
def get_key(text, count=6):
    parser = PlaintextParser.from_string(text, Tokenizer('english'))
    summarizer = TextRankSummarizer()
    summary = summarizer(parser.document, count)
    return [str(sentence) for sentence in summary]

# æ¯ä¸ªå…³é”®å¥åŒ¹é…ç›¸å…³æ®µè½ï¼ˆæ­£æ–‡ï¼‰
def find_support(key_sentences, paragraphs, per_key=3):
    result = []
    for key in key_sentences:
        support = []
        for p in paragraphs:
            if key.split()[0] in p and len(p) > 100:
                support.append(p)
                if len(support) >= per_key:
                    break
        result.append({
            "title": key,
            "content": support
        })
    return result

# Markdown ç¾åŒ–è¾“å‡º
def generate_markdown(book_title, key_ideas, output_path):
    md_lines = []

    # 1. é¡µé¢æ ‡é¢˜
    md_lines.append(f"# ğŸ“˜ Key Ideas from *{book_title}*")
    md_lines.append("")
    md_lines.append("---")
    md_lines.append("")

    # 2. ç›®å½•éƒ¨åˆ†
    md_lines.append(f"## ğŸ§­ Key ideas in *{book_title}*\n")
    for i, idea in enumerate(key_ideas, start=1):
        md_lines.append(f"- [{i}. {idea['title']}](#key-idea-{i})")
    md_lines.append("\n---\n")

    # 3. æ­£æ–‡éƒ¨åˆ†
    for i, idea in enumerate(key_ideas, start=1):
        md_lines.append(f"## ğŸ”¹ Key idea {i} of {len(key_ideas)} <a name='key-idea-{i}'></a>\n")
        md_lines.append(f"### âœ¨ {idea['title']}\n")
        md_lines.append("")
        for para in idea["content"]:
            md_lines.append(para.strip())
            md_lines.append("")
        md_lines.append("---\n")

    # å†™å…¥æ–‡ä»¶
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(md_lines))
    print(f"âœ… Markdown saved to: {output_path}")

# å•ä¸ªæ–‡ä»¶å¤„ç†é€»è¾‘
def summarize_book(txt_path, output_path):
    text = read_book(txt_path)
    paragraphs = split_paragraphs(text)
    key_sentences = get_key(text, count=6)
    key_ideas = find_support(key_sentences, paragraphs)
    book_title = os.path.splitext(os.path.basename(txt_path))[0].replace('_', ' ')
    generate_markdown(book_title, key_ideas, output_path)

# æ‰¹é‡å¤„ç†æ‰€æœ‰ txt
def batch_summarize(input_folder, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    for file in os.listdir(input_folder):
        if file.endswith('.txt'):
            in_path = os.path.join(input_folder, file)
            out_path = os.path.join(output_folder, file.replace('.txt', '_summary.md'))
            summarize_book(in_path, out_path)
            print(f"ğŸ“„ å¤„ç†å®Œæˆï¼š{file}")

# ä¸»ç¨‹åºå…¥å£
if __name__ == '__main__':
    input_folder = 'output_book'
    output_folder = 'book_summaries'
    batch_summarize(input_folder, output_folder)
